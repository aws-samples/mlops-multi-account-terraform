pipeline:
    name: "llm-evaluation-single-model"

dataset:
    dataset_name: "trivia_qa_sampled"
    input_data_location: "evaluation_dataset_trivia.jsonl" #"s3://llmevaluation-smpipelines/tiny_dataset.jsonl"
    dataset_mime_type: "jsonlines"
    model_input_key: "question"
    target_output_key: "answer"

models:
  - name: "llama2-7b"
    model_id: "meta-textgeneration-llama-2-7b"
    model_version: "3.0.0"
    endpoint_name: "llm-eval-meta-textgeneration-llama-2-7b"
    deployment_config:
      instance_type: "ml.g5.2xlarge"
      num_instances: 1
    evaluation_config:
      output: '[0].generated_text'
      content_template: "PROMPT_PLACEHOLDER"
      inference_parameters:
        max_new_tokens: 100
        top_p: 0.9
        temperature: 0.6
      custom_attributes:
        accept_eula: True
      prompt_template: "$feature"
    cleanup_endpoint: True

algorithms:
  - algorithm: "FactualKnowledge"
    module: "fmeval.eval_algorithms.factual_knowledge"
    config: "FactualKnowledgeConfig"
    target_output_delimiter: "<OR>"
